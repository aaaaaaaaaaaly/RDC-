# READ_ME

## 代码思路与功能

## 地穴蠕虫-初阶

- 先导入需要的库

- 爬取的 `askurl`函数：

```
- 通过传入的链接 url和头部信息 head来获取响应
- 通过响应来获取整个网页的代码html
- 返回html
```



- 集合爬取和解析网页内容的`getdata`函数

```
- 传入已经获取的网页代码，头部信息以及存储解析内容的data
- 利用前面写的askurl来获取网页代码html
- 利用soup.find_all来找同时满足‘div’,class='p-parameter'的item
- 写正则表达式lifindlink（就是规则，按照规则来解析）
- 通过规则findlink解析item-->获取data
```



- 主程序

```
- 找到3个要爬的笔记本网页
- 先爬取网页，再解析网页--->getdata()
-返回需要的内容
- 保存到文件
```





## 地穴蠕虫-进阶

- 先导入需要的库
- 获取网页信息的`askurl`函数

```
- 传入要爬取的网页链接url以及头部信息head
- 出现的问题：就是获取的是乱码，经过查找，补充
html=response.text.encode('iso-8859-1').decode('gbk')
解决乱码问题
- 返回获取的网页代码信息html
```



- 观察链接（因为一次要爬取10页，不可能一个一个打）

```
- 一次要爬取10篇作文，那么是需要去寻找链接的规律的
- 首先尝试的是尾部“216265.html”的最后2-3位数字，但是“216266”等等没有作文
- 之后从头部前面来找规律，发现第二位就可以，“226265.html”就有作文信息
- 之后打印出来后，发现只爬取了9篇，原来有一篇的链接是无效的
- 继续按照数字++，后面的有作文，所以修改代码，跳过这一篇没有作文的链接
```



- 集获取网页代码和解析网页的函数`getdata1`函数

```
- 按照前面的观察，以及askurl函数，来获取相关的链接，以及跳过该链接和函数获取的网页代码html
- 解析获取的网页代码html：
- 首先利用Bs，初解析html，获取soup
- 写规则--正则表达式findlink
- 利用规则和网页，来获取需要的部分 elements
- 首先解析题目-正则表达式-findall（规则+解析内容）
- 然后解析作文内容-正则表达式-findall（规则+解析内容）
- 注意，正常爬取的话，正文内容会有链接信息，所以需要一条正则表达式来再筛一遍
 elements_2=re.sub(r'<u>.*?</u>.','',str(elements_2))
- 返回data，里面存放了解析的内容
```



- 主程序

```
- 先获取头部信息
- 设定baseurl，这是作文网所有作文页面都有的一个基础url
- 设定data，传入函数getdata（）
- 获得需要的内容data
- 存入文件
```

