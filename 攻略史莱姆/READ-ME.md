# READ-ME

## 代码思路与功能

## 数据清洗

- 首先导入要清洗的数据，以及库

```python
pd.read_csv("")
```



- 接着观察数据

```python
#观察的几种方法

data[:10]
data.head()
data.describe()
data.columns
data.info()
```



- 查看缺失值

```python
data.isnull()
data.isnull().sum()

data.notnull()
data.info()
```



- 处理缺失值

```python
#本数据我自己只用到用平均值填补
#总的粗略的概括一下

#填充
data[''].fillna(" ")
"""
fillna中可以是自定义的填充值
或者用特征值，比如平均值，中值，众数等
"""

#缺失值上下填充
data[''].fillna(method='ffill')#向上
data[''].fillna(method='bfill)#向下
                
#删除
data.drop("",axis=)
```



- 异常值处理

利用**箱线图**更好点

```python
plt.bar()
dataa.replace({'':},)
```



- 修改列名

```python
data.columns=['','']
```



- 画图观察数据



- 特征分析删除特征

使用到 **特征选择**



- 保存清洗数据到文件

```
data.to_csv("")
```



## 回归模型

- 我分成一元和多元来测试，预测性能



- 导入数据和库，进行基本处理
- 定义要实现的模型的函数

**正规方程推导如下：**

![](https://s1.ax1x.com/2022/04/04/qqPXgU.png)

**梯度下降推导如下：**

![](https://s1.ax1x.com/2022/04/04/qqFlw9.png)

![](https://s1.ax1x.com/2022/04/04/qqF1oR.png)





- 求模型

按照公式来写函数，以底层的数学：拟合线性模型，需要找到最佳的系数theta；

由此引入衡量标准：**最小二乘法**。

在几何上的**直观印象**就是（预测值  ）和（实际值）在竖直方向（平行于y轴方向）的距离最小；即让全体数据量使这个距离平均最小

能达到最小的那个theta就是最合适的theta

- 由此**延伸**出两种算法：
- - 由于 wx **凸函数**，可以通过数学方法公式推导（求导）直接获得最接结果（正规方程）
  - 也可以通过不断迭代的思路，一步步走到全局最低，这个时候学习率的选择很重要（梯度下降）



- 模型评估

**均方误差，可决系数**

公式：
$$
MSE=\frac{\sum^{m}_{i=1}(pre_y-y)^2}{m}
$$
可决系数：
$$
R1=\sum^m_{i=1}(y-pre_y)^2
$$

$$
R2=\sum^m_{i=1}(y-y_{mean})^2
$$

$$
R=1-\frac{R1}{R2}
$$

**残差图**

*看每一种特征和预测对象之间的关系，写了一个for循环*

- 绘图看效果

*用到 plt.scatter，plot，直观观察拟合效果*

- ！特此缩放（**很重要**）

*没特指缩放，和有特指缩放是不一样的，就连数量级的大小也对跑模型的速度有影响。借此查了一下，原来一般的线性回归都是要事先归一化或者标准化一下的，这样会让梯度下降快一点，模型更好一点*



## 结语

以上是基本思路，事实上有debug的很多细节，借用jupyter的直接查看功能一点点改，最终去掉边边角角的东西，呈现这套史莱克攻略~