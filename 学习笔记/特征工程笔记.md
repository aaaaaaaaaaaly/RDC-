# 特征工程

## 算法的认知流程：

1. 概念及工作原理
2. 理论公式及推导
3. 优缺点
4. 应用场景及意义
5. 功能(代码)实现

- 学习链接：[深度了解特征工程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/111296130)

- 特征工程就是发现对因变量y有明显作用的特征，通常称自变量x为特征，特征工程的目标是 **发现重要特征**



![](https://s1.ax1x.com/2022/03/19/qEfLrt.png)

## 数据预处理

分为： **缺失值处理，离群值处理，数据变换**



### 缺失值处理

- 未经处理的原始数据通常有缺失值，离群值
- 缺失值处理：删除，统计值处理，统一值填充，前后值填充，插值法填充，建模预测填充和具体分析

#### 删除

#### 统计值填充

统计值一般指：平均值，中位数，众数，最大值，最小值

#### 统一值填充

#### 前后值填充

使用缺失值前一个或者后一个的值来填充

#### 插值法填充

所谓的插值法，就是在X范围区间中挑选一个或者自定义一个数值，
然后代进去**插值模型**公式当中，求出数值作为缺失值的数据。





### 离散值处理

#### 标准差法

又称为拉依达准则(标准差法)，适用于有较多组数据的时候。

工作原理：它是先假设一组检测数据只含有随机误差，对其进行计算处理得到标准偏差，
按一定概率确定一个区间，认为凡超过这个区间的误差，就不属于随机误差而是粗大误差，
含有该误差的数据应予以剔除。

标准差本身可以体现因子的**离散程度**，是基于因子的平均值μ而定的。在离群值处理过程中，
可通过用μ±nσ来衡量因子与平均值的距离

公式：假设有**近似服从正态分布离散数据**X=[x1,x2,...,xn]，其均值μ与标准差σ分别为：
$$
μ=\frac{\sum^n_{i=1 }x_i}{n},β=(\frac{\sum^n_{i=1}{(x_i-μ)}^{2} }{n})^{1/2}
$$
如何衡量数值是否为离群值？
将区间，[μ−3σ，μ+3σ]的值视为正常值范围，在，[μ−3σ，μ+3σ]外的值视为离群值。





#### MAD法



### 无量纲化

#### 极差标准化

Min-max区间缩放法(**极差标准化**),将数值缩放到[0 1]区间
$$
X_i=\frac{x_i-x_{min}}{x_{max}-x_{min}}
$$

#### 极大值标准化

Max-abs (**极大值标准化**)，标准化之后的每一维特征最大要素为1，其余要素均小于1，理论公式如下：
$$
X_i=\frac{|x_i|}{{|x_{max}|}}
$$

#### 标准差标准化

z-score 标准化(**标准差标准化**)为类似正态分布，均值为0，标准差为1
$$
X_i=\frac{x_i-μ}{β},
$$

$$
μ=\frac{1}{n}\sum^n_{i=0}x_i,
$$

$$
β==(\frac{\sum^n_{i=1}{(x_i-μ)}^{2} }{n})^{1/2}
$$

#### 归一化-总和标准化

归一化(**总和标准化**)，归一化的目的是将所有数据变换成和为1的数据，常用于权重的处理，在不同数据比较中，
常用到权重值来表示其重要性，往往也需要进行加权平均处理。
$$
X_i=\frac{x_i}{\sum^n_{i=1}|x_j|}
$$

#### 非线性归一化

非线性归一化：对于所属范围未知或者所属范围是全体实数，同时不服从正态分布的数据，
对其作Min-max标准化、z-score标准化或者归一化都是不合理的。
要使范围为R的数据映射到区间[0,1]内，需要作一个非线性映射。而常用的有sigmoid函数、arctan函数和tanh函数

**sigmoid函数**
$$
y=\frac{1}{1+e^{-x}}
$$
**arctan函数**
$$
y=arctan x
$$
**tanh函数**
$$
y=tanhx=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

## 特征处理

- 特征处理主要包括：
-  ①数据预处理。即数据的<u>清洗工作</u>，主要为缺失值、异常值、错误值、数据格式、采样度等问题的处理。
-  ②特征转换。即连续变量、离散变量、时间序列等的转换，便于入模。
- 

## 特征构造

- **定义:**特征够着需要根据具体的文艺够着出与目标高度相关的新特征



### 统计值构造法

**概念以及工作原理**

- **概念:**

  指通过统计单个或者多个变量的<u>统计值(max,min,count,mean)</u>等而形成新的特征。



- **单变量：**
  如果某个特征<u>与目标高度相关</u>，那么可以根据具体的情况取这个特征的统计值作为新的特征。



- **多变量：**
  如果特征与特征之间存在交互影响时，那么可以聚合分组两个或多个变量之后，再以统计值构造出新的特征。



### 特征转化

特征也就是我们常常说的变量/自变量，一般分为三类： - 连续型 - 无序类别（离散）型 - 有序类别（离散）型

主要转换方式有以下几种：

![](https://static01.imgkr.com/temp/effc16847e9244958c32664948ad14f9.png)



## 特征抽取

**1）定义**

可能由于特征矩阵过大，一些样本如果直接使用预测模型算法可能在原始数据中有太多的列被建模，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。特征提取是一个自动化的降维过程。使得特征太多的样本被建模的维数降低。

**（2）作用**

最大限度地降低数据的维度的前提下能够同时保证保留目标的重要的信息，特征提取涉及到从原始属性中自动生成一些新的特征集的一系列算法，降维算法就属于这一类。**特征提取是一个自动将观测值降维到一个足够建模的小数据集的过程。** 

 数据降维有以下几点好处：  

1、避免维度灾难，导致算法失效，或者时间复杂度高 

 2、避免高维数据中引入的噪声，防止过拟合 

 3、压缩存储，可视化分析

**（3）方法**

对于列表数据，可使用的方法包括一些投影方法，像主成分分析和无监督聚类算法。 对于图形数据，可能包括一些直线检测和边缘检测，对于不同领域有各自的方法。 特征提取的关键点在于这些方法是自动的（只需要从简单方法中设计和构建得到），还能够解决不受控制的高维数据的问题。大部分的情况下，是将这些不同类型数据（如图，语言，视频等）存成数字格式来进行模拟观察。  不同的数据降维方法除了实现降维目标的作用，同时具有各自的特点，比如主成分分析，降维后的各个特征在坐标上是正交；非负矩阵分解，因为在一些文本，图像领域数据要求非负性，非负矩阵分解在降维的同时保证降维后的数据均非负；字典学习，可以基于任意基向量表示，特征之间不再是独立，或者非负；局部线性嵌入，是一种典型的流型学习方法，具有在一定邻域保证样本之间的距离不变性。

可能由于特征矩阵过大，一些样本如果直接使用预测模型算法可能在原始数据中有太多的列被建模，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。特征提取是一个自动化的降维过程。使得特征太多的样本被建模的维数降低。

![](https://static01.imgkr.com/temp/ae992665548a41e28f2216516a959ce0.png)



## 特征选择

- 学习链接：[【机器学习】特征选择(Feature Selection)方法汇总 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/74198735)

![](https://static01.imgkr.com/temp/5d39d19f63a84179ace976f8597e1296.png)



### 常规流程

特征选择的一般过程：

1. 生成子集：搜索特征子集，为评价函数提供特征子集
2. 评价函数：评价特征子集的好坏
3. 停止准则：与评价函数相关，一般是阈值，评价函数达到一定标准后就可停止搜索
4. 验证过程：在验证数据集上验证选出来的特征子集的有效性

但是， 当特征数量很大的时候， 这个搜索空间会很大，如何找最优特征还是需要一些经验结论。





### 过滤法

- 基本想法是

：分别对每个特征 x_i ，计算 x_i 相对于类别标签 y 的**信息量** S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照<u>从大到小</u>排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y **关联最密切**的一些 特征x_i 。



- 过滤式特征选择的评价标准分为四种，即**距离度量、信息度量、关联度度量以及一致性度量**



#### Pearson相关系数

- 基本思想：

  皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，衡量的是变量之间的线性相关性，结果的取值区间为**[-1,1]** ， -1 表示完全的负相关(这个变量下降，那个就会上升)， +1 表示完全的正相关， 0 表示没有线性相关性。这个方法**只对线性关系敏感**。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近 0 



- 原理实现：先计算**各个特征对目标值**的**相关系数**以及相关系数的**P值** 



- `scipy.stats.pearsonr(x, y)`
  输出:(r, p)
  r:相关系数[-1，1]之间
  p:相关系数显著性

```python
#计算相关系数
def cal_coefficient (data):
    #我要预测的值在columns第一个
    x,y=data.iloc[:,1:],data.iloc[:,0]
    cor_list=[]
    for i in x.columns.tolist():
        cor=np.corrcoef(x[i],y)[0,1]
        cor_list.append(cor)
        print(X.columns.tolist())
        print(cor_list)
    return cor_list
```





- 卡方验证
- 互信息和最大信息系数
- 距离相关系数



#### 方差选择法

- 方差是衡量一个变量的**离散程度**(即数据偏离平均值的程度大小)，
  变量的**方差越大，我们就可以认为它的离散程度越大**，也就是意味着这个变量对模型的贡献和作用



- 思路(先计算各个特征的方差，然后根据**设定的阈值**或**待选择阈值的个数**，选择方差大于阈值的特征),



**数学公式：**

- 计算特征方差的公式：

$$
假设X=[x1,x2,x3,..xn]
$$

$$
var(X)=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-μ)^{2
}
$$

其中，μ是平均值

- 设定阈值，并且筛选出大于阈值的特征或者筛选出待选择阈值的个数4



```python
#进来的data的shape变成m*(n+1)
#代码实现,定义一个方差选择法函数
def var_chose_feature (data,threshold):
    #为防止对原始数表的篡改，现在coppy一份副本
    data1=data.iloc[:,:].copy()
    #先求方差
    var=1/(data1.shape[0]-1)*(np.sum((np.matrix(data1.values)-np.matrix(data1.mean()))**2))
    #再筛选比阈值大的特征
    T=[]
    for index,v in enumerate(var.reshape(-1,1)):
        if v>threshold:
            T.append(index)
    data1=data1.iloc[:,T]
    return var,data1
```



### 包装法

- 基本思想：

  基于hold-out方法，对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集。需要先选定特定算法，通常选用普遍效果较好的算法， 例如Random Forest， SVM， kNN等等。



- 前向传播
- 后向搜索
- 递归特征消除法







### 嵌入法

- 基于<u>惩罚项</u>的特征选择法 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性。





