# 爬虫---学习内容



## 编码规范

![](https://s1.ax1x.com/2022/03/23/qlZF00.png)

## 1.引入模块

![](https://static01.imgkr.com/temp/366007d2113c468ba00779254b168e65.png)





## 封装爬虫的一般代码结构

```python
from bs4 import BeautifulSoup   #网页解析，获取数据
import re                       #正则表达式，进行文字匹配
import urllib.request,urllib.error#制定url，获取网页数据
import xlwt                       #进行excel操作
import sqlite3                   #进行数据库操作



def main():
    baseurl="https://www.jd.com/"
    #爬取网页，解析数据
    datalist=getdata(baseurl)
    savepath = ".\\地穴蠕虫.xls"
    #保存数据
    savedata(savepath)


#爬取网页获取数据,并且解析数据
def getdata(baseurl):
    datalist=[]
    return datalist

#保存数据
def savedata(savepath):
    print(" ")



if __name__=="__main__":
    main()
```





### 爬取网页获取数据解析数据函数

```python
#爬取网页获取数据,并且解析数据
def getdata(baseurl):
    datalist=[]
    #逐一解析数据,在此函数外部定义一个爬取一个网址的函数

    return datalist

#得到一个指定url的网页内容
def askurl(url):
    #指定头部信息，是一个字典  也可以是列表
    # 用户代理表示告诉豆瓣我们是什么类型的机器，浏览器
    #模拟浏览器头部信息，向豆瓣发送消息
    head={"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.46"}

    request=urllib.request.Request(url,headers=head)
    html=""
    try:
        response=urllib.request.urlopen(request)
        #再来解码一下
        html=response.read().decode("utf-8")
        print(html)
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
```



- 现在定义了一个一次爬取一个网页的函数 **askurl**,然后就应该对**getdata函数**进行完善

```python
def getdata(baseurl):
    datalist=[]
    #逐一解析数据,在此函数外部定义一个爬取一个网址的函数

    #askurl函数通过for循环调用25次爬取多个网页
    for i in range(0,10):
        #每一页都有25个电影,但是基础的url是不变的
        url=baseurl+str(i*25)
        html=askurl=(url)
        #这就是获取的一页html,保存获取到的网页源码

        #逐一解析

    return datalist
```







## 2.获取数据

![
](https://static01.imgkr.com/temp/0a7e4a7906b04fcfaabebab80c9dbfc1.png)



### 测试urllib

```python
#测试urllib
import urllib.request


#获取一个get请求
response=urllib.request.urlopen("https://www.baidu.com/")
print(response.read().decode('utf_8'))
#对获取到的网页源码进行utf_8解码


#获取一个post请求
"""
一个专门测试请求是否符合预期的网站：“http://httpbin.org"
"""
import urllib.parse#解析文字
data=bytes(urllib.parse.urlencode({"hello":"world"}),encoding='utf_8')
#写入键值对,对字典进行分装'utf-8'
response2=urllib.request.urlopen("http://httpbin.org/post",data=data)
#print(response2.read().decode("utf_8"))

#再decode一下
"""
用urllib模拟向浏览器发出post请求
要爬取post，就要按照post来分装数据,分装数据通过data来传递一个参数
data传递的参数需要用dytes字节文件来封装，把用户的密码放进去（键值对)
然后用encoding='utf-8'
"""

"""
如果是get请求，就不需要像post请求一样封装
"""

# #提高代码的健壮性，要进行报错预警
try:
#如果没反应，即超时，就自动弹出，防止卡
response3=urllib.request.urlopen("http://httpbin.org/get",timeout=1)
print(response3.read().decode("utf-8"))
except urllib.error.URLError as e:
print("time out")

response3=urllib.request.urlopen("http://httpbin.org/get",timeout=1)
print(response3.status)
#状态码200说明正常访问
#418说明被发现是爬虫
#414说明访问失败



#伪装成浏览器
headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.46"}
url="https://www.douban.com"
#构建了一个请求对象
req=urllib.request.Request(url=url,data=data,headers=headers,method="POST")
#发送请求
response4=urllib.request.urlopen(req)
# print(response4.read().decode("utf-8"))

req2=urllib.request.Request(url=url,headers=headers,method="GET")
response5=urllib.request.urlopen(req2)
print(response5.read().decode("utf-8"))



#post和get请求的区别
"""
post就是携带用户信息的请求方式，get则是根据url直接获取网页信息
post获取的内容直接靠网址是不能获取的，需要提交一些额外的信息，比如查询天气需要输入城市信息
而get获取内容是稳定的
post要发送两个tcp数据包，其想要发送header，服务器返回100，continue了再传送data
"""
```






## 3.解析数据

![](https://static01.imgkr.com/temp/ada70691c58e4f3190da5ba77c2d28d5.png)





### 测试BeautifulSoup

```python
"""
测试一下文档解析BrautifulSoup
将复杂的html文档换成一个复杂的树形结构，每一个节点都是python对象
所有对象可以归纳为4种：
-Tag
-NavigableString
- BeautifulSoup
-Comment
"""

from bs4 import BeautifulSoup
# 模拟已经爬取到文件，现在进行数据解析
file=open("./爬虫.html","rb")
html=file.read()
#用html.parser来解析爬取的html文档
bs=BeautifulSoup(html,"html.parser")

print(bs.title)
print(bs.a)
print(type(bs.head))

# 1.Tag就是标签及其内容:拿到他找到的第一个内容
#如果只想要内容.就直接.string
print(bs.title.string)
print(type(bs.title.string))

# 2.NavigableString就是标签Tag里的内容(字符串)
#可以.a来找到a标签。还可以通过atters来获得该标签内所有的属性值
print(bs.a.atters)


# #3.BeautifulSoup类型,表示整个文档

print(type(bs))
print(bs)

#Comment,是一个特殊的NavigableString，输出的内容不包括注释符号
 print(type(bs.a.string))

#应用，要有针对性地进行处理数据

#文档遍历
#.contents可以帮我们获取标签内特定的内容
print(bs.head.contents)#是列表
print(bs.head.contents[1])

# 遍历文件树
"""
contents:获取Tag的所有子节点，返回一个list
可以用列表的索引来获取他某一个元素

.children获取Tag的所有子节点，返回一个生成器
for child in bs.body.children:
......
"""
#文档的搜索

# (1)find_all()
# 字符串过滤:会查找和字符串完全匹配的内容
t_list=bs.find_all("a")
print(t_list)

#（2）正则表达式搜索
#使用search（）方法来匹配内容
import re
#包含a的就完全被显示，不需要一模一样
t_list=bs.find_all(re.compile("a"))
print(t_list)


# (3)#传入一个函数，根据函数的要求来搜索
def name_is_exists(tag):
    return tag.has_attr("name")
#有name标签的都被打印出来
t_list=bs.find_all(name_is_exists)
print(t_list)


# （4）kwargs 参数,条件筛选
t_list=bs.find_all(class_=True)
for item in t_list:
    print(item)
    

# （5）text
t_list=bs.find_all(text="hao123")
t_list=bs.find_all(text=["hao123","地图",'豆瓣'])
for item in t_list:
    print(item)
    
    
# （6）limit 参数,限定获得多少个
t_list=bs.find_all("a",limit=6)
for item in t_list:
    print(item)
    
#（7） css选择器
t_list=print(bs.select('title'))
#通过标签来查找
for item in t_list:
    print(item)

# （8）css选择器
#用.来指代类名,用类名进行查找

t_list=bs.select('.mnav')
for item in t_list:
    print(item)
```



- `css`

- ```python
  #通过标签查找
  t_list=bs.select('title')
  
  #通过类名查找
  t_list=bs.select(".mnav")
  
  #通过id查找
  t_list=bs.select("#u1")
  
  #通过属性查找
  t_list=bs.select("a[class='bri']")
  
  #通过子标签来查找
  t_list=bs.select("head>title")
  
  ```

  



### 测试Re

- 关于正则表达式

```python
"""
正则表达式：字符串模式
判断字符串是否符合一定的标准

"""
import re
#创建模式对象
pat=re.compile("AA")
#compile生成一个筛选
# 对象此处的AA是正则表达式用来验证其他的字符串

m=pat.search("CBA")
print(m)
#search字符串是被校验的内容，即在pat中寻找"CBA"

n=pat.search("ABCAA")
print(n)

o=pat.search("ABCAADDCCAAA")
print(o)
#发现只会找到第一个符合要求的并且输出，不管后面是否还有符合的


#没有模式对象
#search 前面是模式，后面是要搜索的对象
"""
之前的是
pat=re.compile("规则/模板match")
m=rat.search("被校验的对象“)

现在可以直接简写
m=re.search("规则match“，”被校验的对象““)
"""
m=re.search("asd","Aasd")
print(m)

#find_all前面的字符串是规则，后面的字符串是被校验的字符串
print(re.findall("a","ASDaDFGAa"))

print(re.findall("[A-Z]","ASDaDFGAa"))
#显示的是一个一个符合标准的字符，用列表的形式打印


#这样把符合标准的字符一次性输出，遇到不符合的就断开，接着往下找，之后有符合的依旧如此
print(re.findall("[A-Z]+","ASDaDFGAa"))

#sub起到分隔替换的作用
"""
sub有三个参数:
在第三个字符串中找到第一个字符串是被第二个字符串替换
"""
print(re.sub("a","A","abcdcasd"))

#建议在正则表达式中，被比较的字符串前面加上r,不用担心转义字符的问题
a=r"\aabd-\'"
b="\aabd-\'"
print(a)
print(b)
```





### 补充Re模块

#### 正则表达式的常用操作符

‘![](https://static01.imgkr.com/temp/6979999ef6d94616bb0ea45ef075eb45.png)

![](https://static01.imgkr.com/temp/5f894b14a5814bc2acf20e1a995201c1.png)

- 史上最全的正则表达式链接：[史上最全常用正则表达式大全 - fozero - 博客园 (cnblogs.com)](https://www.cnblogs.com/fozero/p/7868687.html)





#### Re库的主要功能函数

![](https://static01.imgkr.com/temp/1e4a56f7a6c24a5ca93aa2b9d79360bd.png)

- 正则表达式可以包含一些可选标志修饰符来控制匹配的模式；修饰符被指定为一个可选的标志，多个标志可以通过按位OR（|）来指定

如re.I re.M被设置成L，M标志

![](https://static01.imgkr.com/temp/ee81960edb674f17877cd49cdaae57c6.png)





### 正则提取

```python
#定义一个全局变量，就是我们想要比较的字符串，作为一个标准
findLink=re.compile(r'a href="(.*?)">')
#用r读取避免格式化,一个标准是需要观察规律的
#读取影片图片
findImgSrc=re.compile(r'<img.*src="(.*?)"',re.S)#re.S让换行符包含在字符中
#读取影片的片名
findTitle=re.compile(r'<span class="title">(.x-*)</span>')
#读取影片的评分
findRating=re.compile(r'<span class=:rating_nujm" property="v:average">(.*)</span>')
#找到评价人数
findJudge=re.compile(r'<span>(\d*)人评价</span>')
#找到概况
findInq=re.compile(r'<span class ="inq">(.*)</span>')
#找到影片的相关内容
findDq=re.compile(r'<p class=" ">(.*)</p>',re.S)
```



```python
#爬取网页获取数据,并且解析数据
def getdata(baseurl):
    datalist=[]
    #逐一解析数据,在此函数外部定义一个爬取一个网址的函数

    #askurl函数通过for循环调用25次爬取多个网页
    for i in range(0,10):
        #每一页都有25个电影,但是基础的url是不变的
        url=baseurl+str(i*25)
        html=askurl(url)
        #这就是获取的一页html,保存获取到的网页源码
        #datalist.append(html)

        #逐一解析,需要用到BeautifulSoup
        soup=BeautifulSoup(html,"html.parser")
        #用html.parser解析html对象，找到所有的div标签
        for item in soup.find_all('div',class_="item"):
            #查找符合要求的字符串
            """
            找到所有class等于item同时又是div的字符
            形成列表
            保存一部电影的所有信息
            """
            data=[]
            item=str(item)
            #获取详细的连接
            #用re库来通过正则表达式查找指定的字符串
            link=re.findall((findLink,item))[0]
            print(link)
    return datalist
```

